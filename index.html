<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Morteza Mahdiani</title>
    <meta name="author" content="Morteza Mahdiani">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>
<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p class="name" style="text-align: center;">
                                        Morteza Mahdiani
                                    </p>
                                    <p>
                                        I am a Master's student in Computer Science at the <a href='https://diro.umontreal.ca/english/home/'>University of Montreal</a> and <a href='https://mila.quebec/en/directory/morteza-mahdiani'>Mila </a>, currently working in the <a href='https://iancharest.com'>Charest Lab</a>. My research focuses on NeuroAI, computer vision, and generative AI, with a specific interest in comparing AI models with brain data. Previously, I served as a research assistant at <a href='https://mila.quebec/en'>Mila</a> and the <a href='https://www.umontreal.ca/en/'>University of Montreal</a>, working on related interdisciplinary projects in these fields.
                                    </p>
                                    <p>
                                        <strong>Contact:</strong><br>
                                        Email: <a href="mailto:morteza.mahdiani@umontreal.ca">morteza.mahdiani@umontreal.ca</a><br>
                                        Homepage: <a href="https://morteza-mahdiani.github.io/">morteza-mahdiani.github.io</a>
                                    </p>
                                    <p style="text-align:center">
                                        <a href="https://www.linkedin.com/in/morteza-mahdiani/">LinkedIn</a> &nbsp;/&nbsp;
                                        <a href="https://scholar.google.com/citations?user=xFztGO8AAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                                        <a href="https://github.com/morteza-mahdiani">GitHub</a>  &nbsp;/&nbsp;
                                        <a href="https://x.com/janiosadah?lang=en">X</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a href="images/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/IMG_1743.png" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <h2>Recent Publications </h2>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                      <tbody>
                            
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="https://github.com/morteza-mahdiani/OK_CLIP/blob/gh-pages/resources/arch.jpg?raw=true" alt="paper1" width="160" height="160">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://www.cell.com/iscience/fulltext/S2589-0042(24)01522-0">
                                        <span class="papertitle">Fine-grained knowledge about manipulable objects is well-predicted by CLIP</span>
                                    </a>
                                    <br>
                                    Jon Walbrin, Nikita Sossounov, <strong>Morteza Mahdiani</strong>, Igor Vaz, Jorge Almeida
                                    <br>
                                    <em>iScience</em>, 2024
                                    <p>
                                        This study reveals that CLIP-ViT effectively predicts behavioral dimensions of objects, outperforming models trained only on image-based datasets.
                                    </p>
                                </td>
                            </tr>
                            
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/ml_paper_2.png" alt="paper2" width="160" height="160">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://www.biorxiv.org/content/10.1101/2022.05.12.491595v1.abstract">
                                        <span class="papertitle">A Multimodal Neuroimaging Dataset to Study Spatiotemporal Dynamics of Visual Processing in Humans</span>
                                    </a>
                                    <br>
                                    Fatemeh Ebrahiminia, <strong>Morteza Mahdiani</strong>, Seyed-Mahdi Khaligh-Razavi
                                    <br>
                                    <em>bioRxiv</em>, 2022
                                    <p>
                                        This dataset from 21 healthy volunteers combines fMRI and EEG data to study object recognition, offering new insights into human brain function.
                                    </p>
                                </td>
                            </tr>
                            
                        </tbody>
                    </table>
                </td>
            </tr>
        </tbody>
    </table>
</body>
</html>
